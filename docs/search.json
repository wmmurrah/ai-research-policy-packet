[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AI in Research: Policy, SOPs, and Templates",
    "section": "",
    "text": "0.1 Who this is for\nResearchers, PIs, data stewards, model owners, and editors/reviewers affiliated with [INSTITUTION] — [DEPARTMENT].",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview & Quick Start</span>"
    ]
  },
  {
    "objectID": "index.html#quick-start-ten-rules",
    "href": "index.html#quick-start-ten-rules",
    "title": "AI in Research: Policy, SOPs, and Templates",
    "section": "0.2 Quick-start: Ten Rules",
    "text": "0.2 Quick-start: Ten Rules\n\nDon’t upload confidential material (unpublished manuscripts, grants, identifiable data, licensed instruments) to public AI tools.\nHumans are responsible. AI is never an author; disclose substantive AI assistance.\nLog your AI use (tool+version, prompts, inputs by type, outputs kept, human checks).\nPrefer enterprise or local tools approved by [INSTITUTION].\nVerify claims and cite original sources, not the model.\nNo AI for peer review of confidential manuscripts or proposals.\nProtect participants: IRB approval for AI processing; de-identify first.\nDocument datasets and models (Datasheets & Model Cards).\nTrack risks (privacy, bias, IP, security, misuse) and mitigations.\nBe reproducible: save prompts, seeds, code, data versions, and environments.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview & Quick Start</span>"
    ]
  },
  {
    "objectID": "index.html#roles-accountability",
    "href": "index.html#roles-accountability",
    "title": "AI in Research: Policy, SOPs, and Templates",
    "section": "0.3 Roles & accountability",
    "text": "0.3 Roles & accountability\n\nPI: ultimate sign-off on AI use, risk register, and disclosures.\nData Steward: storage, access control, de-identification.\nModel Owner: model card, evaluations, updates.\nProject QA Lead: verifies logs, prompts, reproducibility bundle.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview & Quick Start</span>"
    ]
  },
  {
    "objectID": "index.html#how-the-packet-is-organized",
    "href": "index.html#how-the-packet-is-organized",
    "title": "AI in Research: Policy, SOPs, and Templates",
    "section": "0.4 How the packet is organized",
    "text": "0.4 How the packet is organized\n\npolicy.qmd – Department policy (scope, definitions, roles, permitted/prohibited uses, disclosure, procurement, training, enforcement).\nsop.qmd – Stage-by-stage procedures aligned to the research lifecycle.\nchecklists.qmd – One-page checklists and green/yellow/red lists.\ntemplates.qmd – Disclosure language, IRB snippets, Reviewer attestation, Datasheet & Model Card templates, CSV headers for logs.\n\nappendices.qmd – External references and mappings to national and international guidance.\n\n\n\n\n\n\n\nTip\n\n\n\nSmart defaults for [INSTITUTION]. Enforce a strict ban on public AI use for confidential content, require ICMJE-style disclosure of AI assistance, and adopt NIST AI RMF as the governance spine. Adapt state/funder specifics in Appendix A.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview & Quick Start</span>"
    ]
  },
  {
    "objectID": "policy.html",
    "href": "policy.html",
    "title": "2  Department Policy on AI in Research",
    "section": "",
    "text": "2.1 3. Principles",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Department Policy on AI in Research</span>"
    ]
  },
  {
    "objectID": "policy.html#principles",
    "href": "policy.html#principles",
    "title": "2  Department Policy on AI in Research",
    "section": "",
    "text": "Legality & ethics: Comply with laws, funder rules, publisher policies, and IRB approvals.\nHuman accountability: Researchers retain responsibility for all outputs.\nTransparency: Material AI assistance is disclosed.\nPrivacy & security by design: De-identify early; use approved systems.\nFairness & quality: Measure and mitigate bias; validate claims.\nReproducibility: Preserve artifacts to enable independent verification.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Department Policy on AI in Research</span>"
    ]
  },
  {
    "objectID": "policy.html#roles-responsibilities",
    "href": "policy.html#roles-responsibilities",
    "title": "2  Department Policy on AI in Research",
    "section": "2.2 4. Roles & responsibilities",
    "text": "2.2 4. Roles & responsibilities\n\nPrincipal Investigator (PI): Approves AI use cases; signs risk register and disclosures.\nData Steward: Ensures compliant storage, access control, and de-identification.\nModel Owner: Authors and maintains Model Cards; documents evaluation, updates, and limitations.\nProject QA Lead: Maintains AI Use Logs, prompt archives, change logs, and reproducibility bundles.\nDepartment AI Lead (or designee): Maintains this policy, reviews exceptions, and coordinates training.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Department Policy on AI in Research</span>"
    ]
  },
  {
    "objectID": "policy.html#permitted-vs.-prohibited-uses",
    "href": "policy.html#permitted-vs.-prohibited-uses",
    "title": "2  Department Policy on AI in Research",
    "section": "2.3 5. Permitted vs. prohibited uses",
    "text": "2.3 5. Permitted vs. prohibited uses\n\n2.3.1 5.1 Permitted (with logging)\n\nBrainstorming, outlining, literature scaffolding on public content.\nCopy-editing nonconfidential text; code linting on toy/synthetic data.\nSummarizing public PDFs with proper citation checks.\n\n\n\n2.3.2 5.2 Restricted (require approvals & controls)\n\nData labeling/annotation of de-identified data.\nTranslation of non-sensitive materials.\nTranscription using enterprise tools with approved storage.\n\n\n\n2.3.3 5.3 Prohibited\n\nUploading any confidential content to public AI tools.\nUsing AI to perform peer review of confidential materials.\nPresenting AI-fabricated data as empirical observation.\nGenerating images or figures that could mislead without explicit labeling.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Department Policy on AI in Research</span>"
    ]
  },
  {
    "objectID": "policy.html#disclosure-documentation",
    "href": "policy.html#disclosure-documentation",
    "title": "2  Department Policy on AI in Research",
    "section": "2.4 6. Disclosure & documentation",
    "text": "2.4 6. Disclosure & documentation\nAll material AI assistance must be disclosed in manuscripts/grants (see templates). Projects must maintain: - AI Use Log, Risk Register, Datasheet(s), Model Card(s), and a Reproducibility Bundle (code, lockfiles, seeds, data access notes, prompt files).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Department Policy on AI in Research</span>"
    ]
  },
  {
    "objectID": "policy.html#data-governance-privacy",
    "href": "policy.html#data-governance-privacy",
    "title": "2  Department Policy on AI in Research",
    "section": "2.5 7. Data governance & privacy",
    "text": "2.5 7. Data governance & privacy\n\nApply de-identification at the earliest possible stage.\nStore research data and AI outputs on approved systems.\nRespect licenses and rights (publisher PDFs, test instruments); document TDM legal basis when applicable.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Department Policy on AI in Research</span>"
    ]
  },
  {
    "objectID": "policy.html#security-procurement",
    "href": "policy.html#security-procurement",
    "title": "2  Department Policy on AI in Research",
    "section": "2.6 8. Security & procurement",
    "text": "2.6 8. Security & procurement\n\nPrefer enterprise/private tools approved by [INSTITUTION].\nVendor vetting is required for any tool touching research data.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Department Policy on AI in Research</span>"
    ]
  },
  {
    "objectID": "policy.html#peer-review-editorial-ethics",
    "href": "policy.html#peer-review-editorial-ethics",
    "title": "2  Department Policy on AI in Research",
    "section": "2.7 9. Peer review & editorial ethics",
    "text": "2.7 9. Peer review & editorial ethics\n\nNo public AI tools may access confidential manuscripts or grants.\nIf a venue permits limited AI assistance, it must be private, logged, and disclosed to the venue.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Department Policy on AI in Research</span>"
    ]
  },
  {
    "objectID": "policy.html#training-compliance",
    "href": "policy.html#training-compliance",
    "title": "2  Department Policy on AI in Research",
    "section": "2.8 10. Training & compliance",
    "text": "2.8 10. Training & compliance\n\nAnnual training on AI in research for all researchers and staff.\nAudits may review logs, prompts, risk registers, and artifacts.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Department Policy on AI in Research</span>"
    ]
  },
  {
    "objectID": "policy.html#exceptions",
    "href": "policy.html#exceptions",
    "title": "2  Department Policy on AI in Research",
    "section": "2.9 11. Exceptions",
    "text": "2.9 11. Exceptions\nExceptions require written approval from the Department AI Lead and the PI, with documented mitigations and rationale.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Department Policy on AI in Research</span>"
    ]
  },
  {
    "objectID": "policy.html#enforcement",
    "href": "policy.html#enforcement",
    "title": "2  Department Policy on AI in Research",
    "section": "2.10 12. Enforcement",
    "text": "2.10 12. Enforcement\nViolations may result in corrective actions under [INSTITUTION] policies and sponsor requirements.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Department Policy on AI in Research</span>"
    ]
  },
  {
    "objectID": "sop.html",
    "href": "sop.html",
    "title": "3  Standard Operating Procedures (SOPs)",
    "section": "",
    "text": "This SOP maps the research lifecycle to concrete steps, artifacts, and gates.\n\n3.0.1 Gate 0 — Project registration (before any AI use)\n\nFile an AI Use Case entry: purpose, data types, tools, access, risks, roles.\nCreate initial Risk Register and Reproducibility Bundle skeleton (repo with env.lock, prompts/, logs/).\n\n\n\n3.0.2 Stage 1 — Ideas & literature\n\nUse AI to brainstorm/search; verify against sources.\nArtifact: AI Use Log entries; Source List.\n\n\n\n3.0.3 Stage 2 — Grant/protocol drafting\n\nOnly nonconfidential text may be processed; use enterprise tools.\nArtifact: Disclosure note (if AI used for editing), Access attestations.\n\n\n\n3.0.4 Stage 3 — IRB/ethics & data rights\n\nUpdate protocol to reflect AI processing; include consent language.\nArtifact: IRB-approved language, License/TDM memo.\n\n\n\n3.0.5 Stage 4 — Data collection & curation\n\nDe-identify data; produce Datasheet for Datasets.\nArtifact: Datasheet v1; Data License file; PII risk assessment.\n\n\n\n3.0.6 Stage 5 — Analysis & modeling\n\nUse AI for code suggestions/tests; lock seeds and environments.\nArtifact: Model Card; Evaluation report (accuracy, subgroup fairness, robustness); Change log.\n\n\n\n3.0.7 Stage 6 — Results verification & reporting\n\nIndependent checks; bias & robustness analyses.\nArtifact: QA checklist; Signed verification by QA Lead.\n\n\n\n3.0.8 Stage 7 — Writing & authorship\n\nHuman-led drafting; disclose AI assistance and verification steps.\nArtifact: AI Use Statement in manuscript; prompt archive for major uses.\n\n\n\n3.0.9 Stage 8 — Peer review & editorial work\n\nNo public AI use on confidential content.\nArtifact: Reviewer attestation (if applicable).\n\n\n\n3.0.10 Stage 9 — Publication, sharing & archiving\n\nDeposit code/data (as permitted) with licenses and metadata.\nArtifact: Repository DOI, README, Data/Model Cards, AI Use Log export.\n\n\n\n3.0.11 Stage 10 — Deployment & translation\n\nDefine intended use/out-of-scope; user disclosures; monitoring plan.\nArtifact: Deployment risk assessment, User docs, Incident log.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Standard Operating Procedures (SOPs)</span>"
    ]
  },
  {
    "objectID": "checklists.html",
    "href": "checklists.html",
    "title": "4  Checklists & Allowed Uses",
    "section": "",
    "text": "4.1 One-page PI checklist (printable)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Checklists & Allowed Uses</span>"
    ]
  },
  {
    "objectID": "checklists.html#one-page-pi-checklist-printable",
    "href": "checklists.html#one-page-pi-checklist-printable",
    "title": "4  Checklists & Allowed Uses",
    "section": "",
    "text": "AI Use Case registered; roles named.\nTools are enterprise-approved.\nIRB reflects AI processing; de-identification complete.\nDatasheet(s) and Model Card(s) started.\nRisk Register created; mitigations assigned.\nPrompts & outputs logged; seeds/environments locked.\nDisclosure text prepared.\nBias/robustness tests completed.\nRepository prepared for sharing (licenses, README, DOIs).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Checklists & Allowed Uses</span>"
    ]
  },
  {
    "objectID": "checklists.html#green-yellow-red",
    "href": "checklists.html#green-yellow-red",
    "title": "4  Checklists & Allowed Uses",
    "section": "4.2 Green / Yellow / Red",
    "text": "4.2 Green / Yellow / Red\nGreen (allowed with logging): brainstorming; literature scaffolding; copy-editing nonconfidential text; code linting on toy/synthetic data; alt-text; captions.\nYellow (approval & controls): summarizing public PDFs; de-identified data labeling; translation of non-sensitive materials; enterprise transcription.\nRed (prohibited): confidential manuscript/grant text; identifiable human data to public tools; licensed instruments without permission; automated peer review; AI-fabricated data presented as real; undisclosed AI-generated images.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Checklists & Allowed Uses</span>"
    ]
  },
  {
    "objectID": "checklists.html#reviewereditor-checklist",
    "href": "checklists.html#reviewereditor-checklist",
    "title": "4  Checklists & Allowed Uses",
    "section": "4.3 Reviewer/editor checklist",
    "text": "4.3 Reviewer/editor checklist\n\nI did not use public AI on confidential content.\nAny permitted assistance occurred on private, logged systems.\nI will not retain manuscript text in external tools.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Checklists & Allowed Uses</span>"
    ]
  },
  {
    "objectID": "checklists.html#qa-checklist-analysis-modeling",
    "href": "checklists.html#qa-checklist-analysis-modeling",
    "title": "4  Checklists & Allowed Uses",
    "section": "4.4 QA checklist (analysis & modeling)",
    "text": "4.4 QA checklist (analysis & modeling)\n\nEvaluation includes subgroup performance.\nRobustness/shift tests completed.\nFailure modes documented; limitations section updated.\nModel Card complete; intended use/out-of-scope defined.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Checklists & Allowed Uses</span>"
    ]
  },
  {
    "objectID": "templates.html",
    "href": "templates.html",
    "title": "5  Templates",
    "section": "",
    "text": "5.0.1 Manuscript AI-use disclosure (short)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Templates</span>"
    ]
  },
  {
    "objectID": "templates.html#datasheet-for-datasets-template",
    "href": "templates.html#datasheet-for-datasets-template",
    "title": "5  Templates",
    "section": "5.1 Datasheet for Datasets — template",
    "text": "5.1 Datasheet for Datasets — template\nDataset name\nVersion: v0.1\nOwners: [Name, email]\nProvenance: [Source(s), collection dates]\nLicenses/rights: [Link/terms]\nPopulation/coverage: [Who/what/where/when]\nConsent & lawful basis: [IRB status, consent language, TDM basis]\nSensitive attributes: [List or N/A]\nKnown skews/biases: [Describe]\nPreprocessing & de-ID: [Methods, date, validator]\nQuality checks: [Missingness, noise, audits]\nPermitted uses: [Allowed]\nProhibited uses: [Forbidden]\nRetention/deletion: [Schedule]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Templates</span>"
    ]
  },
  {
    "objectID": "templates.html#model-card-template",
    "href": "templates.html#model-card-template",
    "title": "5  Templates",
    "section": "5.2 Model Card — template",
    "text": "5.2 Model Card — template\nModel name\nVersion: v0.1\nOwner: [Name, email]\nIntended use: [Scope, users, decisions supported]\nOut-of-scope: [Misuse, non-goals]\nTraining data: [Sources, timeframe, datasheet refs]\nEvaluation data: [Datasets, metrics]\nPerformance: [Overall + subgroup]\nRobustness/shift tests: [Methods, results]\nSafety mitigations: [Filters, constraints]\nLimitations: [Caveats]\nUpdate policy: [Schedule, triggers]\nContact: [CONTACT_EMAIL]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Templates</span>"
    ]
  },
  {
    "objectID": "templates.html#ai-use-log-csv-header",
    "href": "templates.html#ai-use-log-csv-header",
    "title": "5  Templates",
    "section": "5.3 AI Use Log — CSV header",
    "text": "5.3 AI Use Log — CSV header\nproject_id,date,stage,tool,tool_version,prompt_file,input_type,\ncontains_confidential(boolean),output_kept(desc),human_verification\n(desc),reviewed_by\n\n5.3.1 Risk Register — CSV header\nproject_id,risk_category,description,likelihood,impact,mitigation,\nowner,status,next_review\n\n\n5.3.2 Prompt archive guidance\n\nSave prompts in prompts/YYYY-MM-DD_context.txt .\nFor long sessions, export transcripts or maintain a summarized prompt file per analysis step.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Templates</span>"
    ]
  },
  {
    "objectID": "general-doc-analysis.html",
    "href": "general-doc-analysis.html",
    "title": "6  AI Research Policy Analysis - General",
    "section": "",
    "text": "6.1 Executive summary (what’s solid today)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>AI Research Policy Analysis - General</span>"
    ]
  },
  {
    "objectID": "general-doc-analysis.html#initial-prompt",
    "href": "general-doc-analysis.html#initial-prompt",
    "title": "6  AI Research Policy Analysis - General",
    "section": "",
    "text": "I would like to conduct a document analysis of existing policies,\npractices, standards and any other guidance for how to use and document \nuse of AI in research. This should be anything that applies to any \nprortion or stage of the research workflow. It should include federal, \nstate, and international sources. The goal is to use this analysis and the \nresulting report to develop standards of practice if they do not exists. \nAct as an expert in AI and research design, analysis, and publishing, with \nadditional expertise in federal funding and federal and state policy. This \nshould be as comprehensive an analysis as possible given the state of \nexisting documentation. I would like to move from general to specific as I \nlearn more about what exists and what is needed. Ask any clarifying \nquestions that would improve the process.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>AI Research Policy Analysis - General</span>"
    ]
  },
  {
    "objectID": "general-doc-analysis.html#executive-summary-whats-solid-today",
    "href": "general-doc-analysis.html#executive-summary-whats-solid-today",
    "title": "6  AI Research Policy Analysis - General",
    "section": "",
    "text": "Peer review: U.S. federal funders explicitly ban using generative AI to read, analyze, or draft peer-review materials; uploading any application/manuscript content to public AI tools violates confidentiality. Enforced at NIH and mirrored widely. (Grants.gov)\nU.S. federal governance: OMB’s AI governance memos now run through M-25-21 (Apr 3, 2025), which replaced M-24-10, while M-25-22 covers AI acquisition. These establish agency-level risk management, inventories, impact controls, and transparency—useful templates for university standards. (The White House)\nReporting & authorship: ICMJE (updated 2024–2025) says AI tools are not authors; substantive use must be acknowledged and verifiable, and journals must instruct reviewers about AI limits. Major publishers (Nature/Science) require disclosure and restrict AI-generated figures/text. (icmje.org)\nRisk management standards: NIST AI RMF 1.0 (with a 2024 Generative AI profile) is the most widely referenced, practical framework for documenting AI risks across the lifecycle; use it as your spine for internal SOPs, audits, and model/data cards. (NIST Publications)\nInternational law & soft law: The EU AI Act (in force; phased timelines through 2025+) exempts AI developed and put into service solely for scientific research, but applies once systems are placed on the market/put into service; prohibits certain practices and adds obligations for high-risk uses. UNESCO & OECD principles give high-level guardrails to align local policy with international norms. (EUR-Lex)\nStates (examples): States are moving on AI governance that can touch public universities: California EO N-12-23 (GenAI program + procurement guidance), New York ITS AI acceptable-use policy, and broader state AI governance (e.g., inventories, restrictions). Colorado’s 2024 AI Act (developers/deployers) is broader market regulation that may affect research deployment to residents. (Governor of California)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>AI Research Policy Analysis - General</span>"
    ]
  },
  {
    "objectID": "general-doc-analysis.html#workflow-matrix-what-to-do-document-and-where-rules-bite",
    "href": "general-doc-analysis.html#workflow-matrix-what-to-do-document-and-where-rules-bite",
    "title": "6  AI Research Policy Analysis - General",
    "section": "6.2 Workflow matrix: what to do & document, and where rules bite",
    "text": "6.2 Workflow matrix: what to do & document, and where rules bite\n\n\n\nResearch stage\nMinimum standard (actionable)\nWhat to record/disclose\n\n\n\n\n0) Governance & planning\nAdopt NIST AI RMF 1.0 + GenAI profile as your internal playbook; define roles (PI, data steward, model owner), risk thresholds, and approval gates.\nRMF alignment table; risk register; approvals; model/data “owner” of record. (NIST Publications)\n\n\n1) Idea & literature review\nIf AI summarizers are used, retain citations & source passages; keep a tool-use log.\nTool + version; prompts; output checkpoints; sources verified by human.\n\n\n2) Grant/protocol drafting\nNo AI may see nonpublic content if sponsor prohibits; keep AI out of confidential sponsor material. Check funder policy.\nStatement of AI assistance limited to generic writing aid (if allowed); confirmation none used on confidential text. (Grants.gov)\n\n\n3) IRB/ethics & data rights\nFor human data, reflect GDPR/HIPAA/Common Rule duties if applicable. If scraping, document legal basis, TDM exceptions, consent language re: AI processing.\nConsent language; lawful basis; data minimization; de-ID plan; DUA/DUC terms permitting AI processing. (EUR-Lex)\n\n\n4) Data collection/curation\nCreate Datasheets for Datasets (who/what/when/how), provenance, and permissible uses; include bias/coverage.\nDatasheet; license terms; PII handling; retention/deletion; data quality checks.\n\n\n5) Model development\nMaintain Model Cards; record training data, preprocessing, eval sets, metrics, failure modes, and guardrails; run bias/robustness tests; align with RMF “Map/Measure/Manage.”\nModel Card; risk tests; change log; dependency hashes; reproducible seeds. (NIST Publications)\n\n\n6) Validation & prereg/reporting\nFor clinical/biomed: follow CONSORT-AI / SPIRIT-AI / TRIPOD-AI / related reporting. Outside biomed, use analogous checklists (Model/Data Cards).\nRegistration; analysis plan; reporting checklist attached to manuscript.\n\n\n7) Writing & authorship\nHumans write & take responsibility. If AI assisted drafting, disclose which sections and how outputs were verified; no AI as author.\n“AI use” paragraph in Acknowledgements/Methods; prompts retained; plagiarism checks performed. (icmje.org)\n\n\n8) Peer review (as author & reviewer)\nDo not upload manuscripts or proposals to AI tools. Reviewers must avoid AI assistance unless journal gives a strict, private, compliant workflow.\nReviewer attestation; journal-specific instructions kept on file. (Grants.gov)\n\n\n9) Publication & sharing\nPublic access (U.S.): follow agency public-access plans (Nelson memo implementation). Share data/code when allowed; include licenses & metadata.\nRepository DOIs; metadata; statements on restrictions/sensitive data. (The White House)\n\n\n10) Deployment/translation\nIf releasing tools/services to users (esp. EU), check EU AI Act scope—research exemption ends when putting into service/on market; follow transparency/high-risk obligations where applicable.\nIntended use; user disclosures; conformity route (if applicable); risk mitigations; post-market monitoring plan. (EUR-Lex)\n\n\n\nNotes on the evidence base & policies cited above\n\nNIH peer-review AI ban (binding): reviewers may not use LLMs; uploading any application content violates confidentiality. (Grants.gov)\nOMB M-25-21 (2025) now governs agency AI use (M-24-10 rescinded); M-25-22 covers acquisition—useful templates for internal governance & vendor vetting. (The White House)\nICMJE: AI cannot be an author; disclose and take responsibility for any AI assistance; give reviewers AI-use guidance. (icmje.org)\nNature/Science: disclosure required; restrictions on AI-generated images and some textual uses. (Nature)\nNIST AI RMF + GenAI profile = best-practice backbone for risk & documentation across the lifecycle. (NIST Publications)\nEU AI Act: research exemption (Recital 25) but obligations trigger when systems are put into service or placed on the market; early prohibitions and penalties phase in during 2025. (EUR-Lex)\nState examples with impact on public universities: CA EO N-12-23 (GenAI program/procurement); NY ITS AI acceptable-use policy. (Governor of California)\n\n\n\n6.2.1 “Starter” Standard Operating Procedure (SOP) & documentation kit\nUse this immediately; we can refine per your department/university.\nA. Governance & registration\n\nRegister each AI use case (project level) with: purpose, data, model(s), access controls, and a named model owner. Map to NIST RMF functions (Govern/Map/Measure/Manage). (NIST Publications)\nMaintain an AI risk register (privacy, bias, robustness, misuse, IP/copyright), with mitigations and sign-offs at each milestone. (NIST Publications)\n\nB. Dataset documentation\n\nCreate a Datasheet for Datasets for every dataset (source, consent, licenses, sensitive attributes, known skews, allowed AI uses).\nRecord TDM (text-and-data-mining) legal basis and opt-outs where applicable (esp. EU TDM rules). (EUR-Lex)\n\nC. Model documentation\n\nPublish Model Cards (intended use, data, metrics, failure modes, safety mitigations, update policy).\nKeep full reproducibility bundle: code, environment, seeds, data access notes.\n\nD. Authorship & manuscript\n\nInclude an AI use statement (template):\n\n\n“The authors used &lt;tool & version&gt; to &lt;copy-edit/translate/summarize&gt; sections &lt;X&gt;. Outputs were reviewed and edited by the authors, and all accuracy, originality, and citation responsibilities remain with the authors. No AI systems had access to nonpublic or confidential data/manuscripts.”\n\nAlign with journal/funder requirements (ICMJE, Nature/Science). (icmje.org)\nE. Peer review\n\nStrictly no AI on confidential materials (grant proposals, manuscripts) unless the journal/funder provides a private, approved tool and explicit permission. (Grants.gov)\n\nF. Public access & sharing\n\nComply with your funder’s public-access plan (Nelson memo implementation): deposit publications and data (or give justified exceptions). Provide machine-readable metadata & PIDs. (The White House)\n\nG. Deployment\n\nIf you release a model/app beyond research, assess EU AI Act/other jurisdictional obligations (transparency, risk management, high-risk conformity). Document “intended use,” user notices, and monitoring. (EUR-Lex)\n\n\n\n\n6.2.2 Gaps & where to set your own standard\n\nSocial/education research outside biomed lacks CONSORT-AI-style, field-specific reporting. Use Model/Data Cards + NIST RMF to fill the gap until discipline-specific extensions emerge. (NIST Publications)\nState policy patchwork: campus-level acceptable-use and procurement rules may be stricter than funders’ policies—align early with state IT policy (e.g., NY ITS) and your General Counsel. (IT Services)\nReviewer guidance varies by publisher; default to the NIH/ICMJE floor: no public AI tools on confidential content; if any assistance is allowed by a journal, document it and keep it local/private. (Grants.gov)\n\n\n\n\n6.2.3 Suggested deliverables (I can draft next)\n\nDepartmental AI in Research Policy (10 pages): scope, definitions, governance roles, prohibited uses, disclosures, audit.\nOne-page PI checklist (by stage).\nTemplates: AI use statement; Dataset Datasheet; Model Card; Risk register; Reviewer attestation.\n\n\n\n\n6.2.4 Smart defaults for Auburn/EFLT (today)\n\nAdopt NIST AI RMF 1.0 + GenAI profile as the lab/department standard. (NIST Publications)\nEnforce NIH-style peer-review ban on AI for all internal and external reviews. (Grants.gov)\nRequire ICMJE-style disclosure for any AI assistance in manuscripts/grants. (icmje.org)\nFor projects touching the EU (data or deployment), track EU AI Act timelines and exemptions. (EUR-Lex)\nAlign with state-level acceptable-use (e.g., NY ITS example) for university-owned devices/accounts; restrict public AI tools for sensitive data. (IT Services)\n\n\n\n\n6.2.5 Clarifying questions (answer any/all and I’ll tailor the next iteration)\n\nScope of data & populations: Human subjects only, or also web-scraped/administrative data? Any EU/UK participants (GDPR) or clinical collaborations (HIPAA/FDA)?\nWhere do you plan to deploy models (beyond publications)—internal dashboards only, or public-facing tools/apps (triggers EU AI Act-like duties)? (EUR-Lex)\nPublisher mix: Which journals/presses do you submit to most (ICMJE-aligned vs. social-science outlets), so I can map exact editorial policies? (icmje.org)\nCampus governance: Do you want the policy written for your department, College of Education, or as a university-wide recommendation that references state IT policy (e.g., CA/NY-style AU)? (IT Services)\nTool posture: Any enterprise AI tools (private instances) available on campus (e.g., MS Copilot, ChatGPT Teams/Enterprise, Claude for Teams) that we should whitelist for limited uses?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>AI Research Policy Analysis - General</span>"
    ]
  },
  {
    "objectID": "appendices.html",
    "href": "appendices.html",
    "title": "7  Appendices & External References",
    "section": "",
    "text": "7.0.1 Appendix A — External guidance to align with (curate per your use)\n\nNational/International: NIST AI RMF; ICMJE authorship & AI guidelines; discipline-specific reporting (e.g., CONSORT-AI/SPIRIT-AI/TRIPOD-AI in biomed); EU AI Act research exemption vs. deployment obligations; OECD/UNESCO principles.\nU.S. Federal: Sponsor and agency rules on AI use for peer review and confidentiality (e.g., NIH); agency public-access plans (article + data).\nState & Institutional: State IT AI acceptable-use/procurement; [INSTITUTION] vendor vetting; campus data classification & storage.\n\n\nAction: Replace this list with citations/links applicable to [STATE] and your typical funders (e.g., NIH/IES/NSF). Add any journal-specific policies you frequently encounter.\n\n\n\n7.0.2 Appendix B — Mapping table\n\n\n\nExternal rule/guidance\nWhat it says\nOur policy hook\n\n\n\n\n[Source]\n[Summary]\n[Policy section & artifact]\n\n\n\n\n\n7.0.3 Appendix C — Glossary\nPlain-language definitions for AI, GenAI, confidential materials, de-identification, TDM, bias/fairness, robustness, model card, datasheet, etc.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Appendices & External References</span>"
    ]
  }
]